name: Lightspeed Core Service (LCS)
service:
  host: 0.0.0.0
  port: 8080

  auth_enabled: false
  workers: 1
  color_log: true
  access_log: true
llama_stack:
  use_as_library_client: true
  library_client_config_path: run.yaml
user_data_collection:
  feedback_enabled: true
  feedback_storage: "/tmp/data/feedback"
  transcripts_enabled: true
  transcripts_storage: "/tmp/data/transcripts"
authentication:
  module: "noop"
conversation_cache:
  type: "sqlite"
  sqlite:
    db_path: "/tmp/data/conversation-cache.db"
inference:
  default_model: gpt-4o-mini
  default_provider: openai
mcp_servers:
  - name: "obs"
    provider_id: "model-context-protocol"
    url: "http://host.containers.internal:9100/mcp"
  - name: "kube"
    provider_id: "model-context-protocol"
    url: "http://host.containers.internal:8081/mcp"
  - name: "ngui"
    provider_id: "model-context-protocol"
    url: "http://host.containers.internal:9200/mcp"

customization:
  system_prompt: |-
    Always use available tools.
